import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import skipgrams
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sample data
sentences = ["this is a sample sentence", "word2vec is an interesting model"]

# Tokenize the sentences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

# Generate skip-grams
skip_grams = [skipgrams(sequence, vocabulary_size=vocab_size, window_size=5) for sequence in sequences]

# Flatten the skip-grams
pairs, labels = [], []
for sg in skip_grams:
    if sg is None:
        continue
    pair, label = sg
    pairs.extend(pair)
    labels.extend(label)

pairs = np.array(pairs)
labels = np.array(labels)

# Define the Word2Vec model
embedding_dim = 300

input_target = tf.keras.layers.Input((1,))
input_context = tf.keras.layers.Input((1,))

embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1, name='embedding')
target = embedding(input_target)
target = tf.keras.layers.Reshape((embedding_dim,))(target)
context = embedding(input_context)
context = tf.keras.layers.Reshape((embedding_dim,))(context)

dot_product = tf.keras.layers.Dot(axes=1)([target, context])
output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)

model = tf.keras.models.Model(inputs=[input_target, input_context], outputs=output)
model.compile(loss='binary_crossentropy', optimizer='adam')

# Train the model
target_input = pairs[:, 0]
context_input = pairs[:, 1]

model.fit([target_input, context_input], labels, epochs=20, batch_size=128)

# Get the embedding weights
word_vectors = model.get_layer('embedding').get_weights()[0]

# Save the embeddings
np.save('word_vectors.npy', word_vectors)

# To use the embeddings:
# word_vectors = np.load('word_vectors.npy')
